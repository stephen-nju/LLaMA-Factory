./mllmtrain.sh --name Qwen2-VL-2B-Instruct_rym_ner_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_rym_ner_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_common_rym_ner_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset common_rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_common_rym_ner_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset common_rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_yjzl_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset yjzl_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_yjzl_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset yjzl_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_thzy_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset thzy_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_thzy_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset thzy_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_rccq_ttrain_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rccq_ttrain --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_rccq_ttrain_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rccq_ttrain --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_gjccq_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset gjccq_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_gjccq_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset gjccq_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_tzjx_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset tzjx_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_tzjx_train_lora=_finetuning_typelora_lora_rank32_lora_targetall_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset tzjx_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_rym_ner_train_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_rym_ner_train_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_common_rym_ner_train_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset common_rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_common_rym_ner_train_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset common_rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_yjzl_train_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset yjzl_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_yjzl_train_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset yjzl_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_thzy_train_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset thzy_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_thzy_train_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset thzy_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_rccq_ttrain_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rccq_ttrain --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_rccq_ttrain_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rccq_ttrain --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_gjccq_train_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset gjccq_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_gjccq_train_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset gjccq_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_tzjx_train_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset tzjx_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_tzjx_train_dora=_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_use_doraTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset tzjx_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --use_dora True 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_rym_ner_train_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_rym_ner_train_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_common_rym_ner_train_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset common_rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_common_rym_ner_train_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset common_rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_yjzl_train_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset yjzl_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_yjzl_train_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset yjzl_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_thzy_train_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset thzy_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_thzy_train_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset thzy_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_rccq_ttrain_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rccq_ttrain --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_rccq_ttrain_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rccq_ttrain --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_gjccq_train_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset gjccq_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_gjccq_train_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset gjccq_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_tzjx_train_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset tzjx_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_tzjx_train_loraplus=_finetuning_typelora_lora_alpha64_lora_dropout0.1_lora_rank32_lora_targetall_loraplus_lr_embedding1e-06_loraplus_lr_ratio32_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset tzjx_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_alpha 64 --lora_dropout 0.1 --lora_rank 32 --lora_target all --loraplus_lr_embedding 1e-06 --loraplus_lr_ratio 32 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_rym_ner_train_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convetFalse_pissa_initFalse_pissa_iter16_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convet False --pissa_init False --pissa_iter 16 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_rym_ner_train_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convetFalse_pissa_initFalse_pissa_iter16_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convet False --pissa_init False --pissa_iter 16 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_common_rym_ner_train_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convetFalse_pissa_initFalse_pissa_iter16_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset common_rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convet False --pissa_init False --pissa_iter 16 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_common_rym_ner_train_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convetFalse_pissa_initFalse_pissa_iter16_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset common_rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convet False --pissa_init False --pissa_iter 16 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_yjzl_train_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convetFalse_pissa_initFalse_pissa_iter16_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset yjzl_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convet False --pissa_init False --pissa_iter 16 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_yjzl_train_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convetFalse_pissa_initFalse_pissa_iter16_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset yjzl_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convet False --pissa_init False --pissa_iter 16 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_thzy_train_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convetFalse_pissa_initFalse_pissa_iter16_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset thzy_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convet False --pissa_init False --pissa_iter 16 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_thzy_train_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convetFalse_pissa_initFalse_pissa_iter16_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset thzy_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convet False --pissa_init False --pissa_iter 16 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_rccq_ttrain_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convetFalse_pissa_initFalse_pissa_iter16_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rccq_ttrain --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convet False --pissa_init False --pissa_iter 16 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_rccq_ttrain_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convetFalse_pissa_initFalse_pissa_iter16_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rccq_ttrain --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convet False --pissa_init False --pissa_iter 16 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_gjccq_train_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convetFalse_pissa_initFalse_pissa_iter16_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset gjccq_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convet False --pissa_init False --pissa_iter 16 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_gjccq_train_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convetFalse_pissa_initFalse_pissa_iter16_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset gjccq_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convet False --pissa_init False --pissa_iter 16 
wait
./mllmtrain.sh --name Qwen2-VL-2B-Instruct_tzjx_train_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convetFalse_pissa_initFalse_pissa_iter16_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset tzjx_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convet False --pissa_init False --pissa_iter 16 
wait
./mllmtrain.sh --name Qwen2-VL-7B-Instruct_tzjx_train_pissa=_finetuning_typelora_lora_rank32_lora_targetall_pissa_convetFalse_pissa_initFalse_pissa_iter16_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset tzjx_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type lora --lora_rank 32 --lora_target all --pissa_convet False --pissa_init False --pissa_iter 16 
wait
./mllmtrain_torchrun.sh --name Qwen2-VL-2B-Instruct_rym_ner_train_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True 
wait
./mllmtrain_torchrun.sh --name Qwen2-VL-7B-Instruct_rym_ner_train_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True 
wait
./mllmtrain_torchrun.sh --name Qwen2-VL-2B-Instruct_common_rym_ner_train_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset common_rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True 
wait
./mllmtrain_torchrun.sh --name Qwen2-VL-7B-Instruct_common_rym_ner_train_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset common_rym_ner_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True 
wait
./mllmtrain_torchrun.sh --name Qwen2-VL-2B-Instruct_yjzl_train_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset yjzl_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True 
wait
./mllmtrain_torchrun.sh --name Qwen2-VL-7B-Instruct_yjzl_train_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset yjzl_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True 
wait
./mllmtrain_torchrun.sh --name Qwen2-VL-2B-Instruct_thzy_train_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset thzy_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True 
wait
./mllmtrain_torchrun.sh --name Qwen2-VL-7B-Instruct_thzy_train_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset thzy_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True 
wait
./mllmtrain_torchrun.sh --name Qwen2-VL-2B-Instruct_rccq_ttrain_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rccq_ttrain --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True 
wait
./mllmtrain_torchrun.sh --name Qwen2-VL-7B-Instruct_rccq_ttrain_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset rccq_ttrain --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True 
wait
./mllmtrain_torchrun.sh --name Qwen2-VL-2B-Instruct_gjccq_train_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset gjccq_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True 
wait
./mllmtrain_torchrun.sh --name Qwen2-VL-7B-Instruct_gjccq_train_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset gjccq_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True 
wait
./mllmtrain_torchrun.sh --name Qwen2-VL-2B-Instruct_tzjx_train_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset tzjx_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-2B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True 
wait
./mllmtrain_torchrun.sh --name Qwen2-VL-7B-Instruct_tzjx_train_galore=_finetuning_typefull_galore_layerwiseFalse_galore_proj_typestd_galore_scale0.25_galore_targetall_galore_update_interval200_use_galoreTrue_maxseq2048_lr0.0001 --batch_size 4 --cutoff_len 2048 --dataset tzjx_train --do_train True --epochs 3 --gradient_accumulation_steps 1 --include node4 --lr 0.0001 --model_name_or_path /home/jovyan/zhubin/DATA/models/Qwen2-VL-7B-Instruct --template qwen2_vl --preprocessing_num_workers 16 --stage sft --finetuning_type full --galore_layerwise False --galore_proj_type std --galore_scale 0.25 --galore_target all --galore_update_interval 200 --use_galore True 
wait
